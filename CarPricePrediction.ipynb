# 1) Setup
import os
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime

from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import mean_squared_error, r2_score

plt.style.use('seaborn-v0_8')
sns.set_context('notebook')

# Paths (edit DATA_RAW if your file is elsewhere)
DATA_RAW = os.path.join('data/vehicles.csv')
DATA_PROCESSED = os.path.join('data/processed/cleaned_vehicles.csv')
print(DATA_PROCESSED)
FIG_DIR = os.path.join('reports/figures')
FINDINGS_MD = os.path.join('reports/findings.md')

os.makedirs(os.path.dirname(DATA_PROCESSED), exist_ok=True)
os.makedirs(FIG_DIR, exist_ok=True)

print('Raw data path:', DATA_RAW)
print('Figures path:', FIG_DIR)


# 2) Load data
try:
    df = pd.read_csv(DATA_RAW)
except FileNotFoundError:
    raise FileNotFoundError("vehicles.csv not found. Place it in data/raw/vehicles.csv relative to this notebook.")

# Normalize column names: strip, lowercase
df.columns = [c.strip().lower() for c in df.columns]

expected_cols = [
    'id','region','price','year','manufacturer','model','condition','cylinders','fuel',
    'odometer','title_status','transmission','vin','drive','size','type','paint_color','state'
]
missing = [c for c in expected_cols if c not in df.columns]
if missing:
    print('Warning: Missing expected columns:', missing)

print('Rows:', len(df), '| Columns:', len(df.columns))
df.head()


# 3) Cleaning & feature engineering

df = df.drop_duplicates()

# Remove rows with missing or non-positive price
df = df[df['price'].notna()]
df = df[df['price'] > 0]

# Year sanity-check and derive age
now_year = datetime.now().year
df = df[df['year'].between(1980, now_year+1)]
df['age'] = now_year - df['year']

# Odometer sanity-check
df = df[df['odometer'].notna()]
df = df[df['odometer'] >= 0]

# Parse cylinders: e.g., "8 cylinders" -> 8
def parse_cyl(x):
    if pd.isna(x):
        return np.nan
    try:
        return float(str(x).split()[0])
    except Exception:
        return np.nan
df['cylinders_num'] = df['cylinders'].apply(parse_cyl)

# Winsorize price and odometer to reduce extreme outliers (1%/99%)
for col in ['price','odometer']:
    lo, hi = df[col].quantile([0.01, 0.99])
    df = df[(df[col] >= lo) & (df[col] <= hi)]

# Save cleaned data
df.to_csv(DATA_PROCESSED, index=False)
print('Cleaned rows:', len(df))


# 4) Exploratory data analysis & visualizations

# Distribution of price
fig, ax = plt.subplots(figsize=(7,4))
sns.histplot(df['price'], kde=True, ax=ax)
ax.set_title('Distribution of Price')
ax.set_xlabel('Price')
fig.tight_layout(); fig.savefig(os.path.join(FIG_DIR, '01_dist_price.png'))

# Log-price distribution
fig, ax = plt.subplots(figsize=(7,4))
sns.histplot(np.log1p(df['price']), kde=True, ax=ax)
ax.set_title('Distribution of Log(Price)')
ax.set_xlabel('log1p(Price)')
fig.tight_layout(); fig.savefig(os.path.join(FIG_DIR, '02_dist_log_price.png'))

# Scatter: odometer vs price
fig, ax = plt.subplots(figsize=(7,4))
sns.scatterplot(data=df, x='odometer', y='price', alpha=0.4, ax=ax)
ax.set_title('Odometer vs Price')
ax.set_xlabel('Odometer (miles)'); ax.set_ylabel('Price')
fig.tight_layout(); fig.savefig(os.path.join(FIG_DIR, '03_odometer_vs_price.png'))

# Boxplot: price by transmission (top categories)
top = df['transmission'].value_counts().nlargest(6).index
fig, ax = plt.subplots(figsize=(8,4))
sns.boxplot(data=df[df['transmission'].isin(top)], x='transmission', y='price', ax=ax)
ax.set_title('Price by Transmission')
ax.tick_params(axis='x', rotation=30)
fig.tight_layout(); fig.savefig(os.path.join(FIG_DIR, '04_price_by_transmission.png'))

# Boxplot: price by fuel
top = df['fuel'].value_counts().nlargest(6).index
fig, ax = plt.subplots(figsize=(8,4))
sns.boxplot(data=df[df['fuel'].isin(top)], x='fuel', y='price', ax=ax)
ax.set_title('Price by Fuel Type')
ax.tick_params(axis='x', rotation=30)
fig.tight_layout(); fig.savefig(os.path.join(FIG_DIR, '05_price_by_fuel.png'))

# Boxplot: price by manufacturer (top 12)
top_mfg = df['manufacturer'].value_counts().nlargest(12).index
fig, ax = plt.subplots(figsize=(10,5))
sns.boxplot(data=df[df['manufacturer'].isin(top_mfg)], x='manufacturer', y='price', ax=ax)
ax.set_title('Price by Manufacturer (Top 12)')
ax.tick_params(axis='x', rotation=30)
fig.tight_layout(); fig.savefig(os.path.join(FIG_DIR, '06_price_by_manufacturer.png'))

# Correlation heatmap for numeric features
num_cols = [c for c in ['price','year','age','odometer','cylinders_num'] if c in df.columns]
corr = df[num_cols].corr(numeric_only=True)
fig = plt.figure(figsize=(6,5))
sns.heatmap(corr, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap (Numeric Features)')
plt.tight_layout(); plt.savefig(os.path.join(FIG_DIR, '07_corr_heatmap.png'))

print('Figures saved to', FIG_DIR)


# 5) Modeling with imputers to prevent NaN errors
TARGET = 'price'
X = df.drop(columns=[TARGET])
y = pd.to_numeric(df[TARGET], errors='coerce')

# Feature lists
numeric_features = [c for c in ['age','odometer','cylinders_num'] if c in X.columns]
categorical_features = [c for c in [
    'region','manufacturer','model','condition','cylinders','fuel','title_status',
    'transmission','drive','size','type','paint_color','state'] if c in X.columns]


# Drop identifiers
X = X.drop(columns=[c for c in ['id','vin'] if c in X.columns], errors='ignore')

# Guardrails: drop rows with missing target or non-positive
mask = y.notna() & (y > 0)
X = X.loc[mask].copy(); y = y.loc[mask].copy()

# Cast categoricals to string to avoid dtype issues
for c in categorical_features:
    X[c] = X[c].astype(str)

# Diagnostics
print('Rows after target filtering:', len(X))
print('NaNs in X before impute:', X.isna().sum().sum())
print('NaNs in y:', pd.isna(y).sum())

# Dynamic CV folds
n_rows = len(X)
cv_folds = 5 if n_rows >= 50 else max(3, min(5, n_rows // 2))
cv = KFold(n_splits=cv_folds, shuffle=True, random_state=42)
print(f'Using cv={cv_folds}')

# Preprocessing with imputers

# Imputers (add this line)
from sklearn.impute import SimpleImputer


numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('ohe', OneHotEncoder(handle_unknown='ignore', drop='first'))
])

preprocess = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='drop'
)

# Baseline Linear Regression
lin_pipe = Pipeline(steps=[('prep', preprocess), ('model', LinearRegression())])

# Try single fit first to surface issues
try:
    lin_pipe.fit(X.sample(min(200, len(X))), y.sample(min(200, len(y))))
    print('Single fit OK')
except Exception as e:
    raise RuntimeError(f'Single fit failed: {type(e).__name__}: {e}')

# Cross-validation
r2_scores = cross_val_score(lin_pipe, X, y, cv=cv, scoring='r2')
rmse_scores = -cross_val_score(lin_pipe, X, y, cv=cv, scoring='neg_root_mean_squared_error')
print(f'Linear Regression CV R²: {r2_scores.mean():.3f} ± {r2_scores.std():.3f}')
print(f'Linear Regression CV RMSE: {rmse_scores.mean():.2f} ± {rmse_scores.std():.2f}')

# Regularized models with GridSearch
ridge_pipe = Pipeline([('prep', preprocess), ('model', Ridge())])
lasso_pipe = Pipeline([('prep', preprocess), ('model', Lasso(max_iter=5000))])
enet_pipe  = Pipeline([('prep', preprocess), ('model', ElasticNet(max_iter=5000))])

param_grid_ridge = {'model__alpha': np.logspace(-3, 3, 20)}
param_grid_lasso = {'model__alpha': np.logspace(-3, 3, 20)}
param_grid_enet  = {'model__alpha': np.logspace(-3, 3, 10), 'model__l1_ratio': np.linspace(0.1, 0.9, 9)}
scoring = 'neg_root_mean_squared_error'

gs_ridge = GridSearchCV(ridge_pipe, param_grid_ridge, cv=cv, scoring=scoring, n_jobs=-1)
gs_lasso = GridSearchCV(lasso_pipe, param_grid_lasso, cv=cv, scoring=scoring, n_jobs=-1)
gs_enet  = GridSearchCV(enet_pipe,  param_grid_enet,  cv=cv, scoring=scoring, n_jobs=-1)

print('Fitting Ridge...'); gs_ridge.fit(X, y)
print('Fitting Lasso...'); gs_lasso.fit(X, y)
print('Fitting ElasticNet...'); gs_enet.fit(X, y)

# Report best
best_gs = min([gs_ridge, gs_lasso, gs_enet], key=lambda gs: -gs.best_score_)
best_name = 'Ridge' if best_gs is gs_ridge else ('Lasso' if best_gs is gs_lasso else 'ElasticNet')
print('Selected best model:', best_name)

best_pipe = best_gs.best_estimator_
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
best_pipe.fit(X_train, y_train)
y_pred = best_pipe.predict(X_test)

from sklearn.metrics import r2_score, mean_squared_error
print(f'Test R²: {r2_score(y_test, y_pred):.3f}')
print(f'Test RMSE: {mean_squared_error(y_test, y_pred, squared=False):.2f}')


# 6) Coefficient interpretation (robust)

# Extract feature names from the fitted pipeline
prep = best_pipe.named_steps['prep']
model = best_pipe.named_steps['model']

# Numeric names
num_names = prep.transformers_[0][2]

# Categorical names from inner OHE inside the categorical transformer pipeline
ohe = prep.transformers_[1][1].named_steps['ohe']
cat_base = prep.transformers_[1][2]
cat_names = ohe.get_feature_names_out(cat_base).tolist()

# Combine names
feature_names = list(num_names) + cat_names

# Build a DataFrame of coefficients, sorted by magnitude
import pandas as pd
coefs = pd.Series(model.coef_, index=feature_names)
coef_df = coefs.reset_index().rename(columns={'index':'feature', 0:'coef'})
coef_df = coef_df.sort_values('coef', ascending=False)

# Save coefficients table — ensure directory exists
coef_path = os.path.join('..', 'reports', 'coef_table.csv')
os.makedirs(os.path.dirname(coef_path), exist_ok=True)
coef_df.to_csv(coef_path, index=False, encoding='utf-8')
print('Top positive drivers (increase price):')
print(coef_df.head(12))
print('\nTop negative drivers (decrease price):')
print(coef_df.tail(12))
print(f'\nSaved coefficients to: {coef_path}')



# 7) Findings summary (robust and self-contained)

import os
import pandas as pd
from sklearn.metrics import r2_score, mean_squared_error

# --- Ensure we have coefficient table variables; recompute if needed ---
prep = best_pipe.named_steps['prep']
model = best_pipe.named_steps['model']

# Numeric feature names
num_names = prep.transformers_[0][2]

# Categorical feature names from inner OneHotEncoder
ohe = prep.transformers_[1][1].named_steps['ohe']
cat_base = prep.transformers_[1][2]
cat_names = ohe.get_feature_names_out(cat_base).tolist()

feature_names = list(num_names) + cat_names

# Align coefficients to names
coefs = pd.Series(model.coef_, index=feature_names)

# Create and sort coefficient table
coef_df = coefs.reset_index().rename(columns={'index':'feature', 0:'coef'}).sort_values('coef', ascending=False)

# Top drivers
top_pos = coef_df.head(12)
top_neg = coef_df.tail(12)

# --- Ensure reports dir exists and save the coefficient table ---
coef_path = os.path.join('reports/coef_table.csv')
os.makedirs(os.path.dirname(coef_path), exist_ok=True)
coef_df.to_csv(coef_path, index=False, encoding='utf-8')

# --- Build summary text ---
summary = (
    f"# Findings: Price Drivers in Vehicles Dataset\n\n"
    f"**Best model:** {best_name}\n\n"
    f"**Cross-validated performance** (RMSE lower is better):\n"
    f"- Ridge best RMSE = {-gs_ridge.best_score_:.2f}\n"
    f"- Lasso best RMSE = {-gs_lasso.best_score_:.2f}\n"
    f"- ElasticNet best RMSE = {-gs_enet.best_score_:.2f}\n\n"
    f"**Hold-out performance:**\n"
    f"- Test R² = {r2_score(y_test, y_pred):.3f}\n"
    f"- Test RMSE = {mean_squared_error(y_test, y_pred, squared=False):.2f}\n\n"
    f"**Key drivers that INCREASE price (top 12 coefficients):**\n\n{top_pos.to_string(index=False)}\n\n"
    f"**Key drivers that DECREASE price (bottom 12 coefficients):**\n\n{top_neg.to_string(index=False)}\n\n"
    "**Interpretation:**\n"
    "- Mileage (odometer) and vehicle age typically **decrease** price.\n"
    "- Premium manufacturers, specific models/trims, and certain transmission/fuel types may **increase** price.\n\n"
    "**Recommendations:**\n"
    "- Use model outputs to set pricing bands by age/mileage and adjust for manufacturer/model premiums.\n"
    "- Review listings with unusually low/high predictions for potential data quality issues or outliers.\n\n"
    "**Next steps:**\n"
    "- Add features: trim level, engine size, options, accident history, and geographic demand.\n"
    "- Try tree-based models (RandomForest, XGBoost) and SHAP for non-linear effects.\n"
)

# --- Write the findings to disk ---
FINDINGS_MD = os.path.join('reports/findings.md')
os.makedirs(os.path.dirname(FINDINGS_MD), exist_ok=True)
with open(FINDINGS_MD, 'w', encoding='utf-8') as f:
    f.write(summary)

print('Findings written to:', FINDINGS_MD)
print('Saved coefficients to:', coef_path)
